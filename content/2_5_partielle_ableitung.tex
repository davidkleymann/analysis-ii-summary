\subsection{Partielle Ableitung (Partial Derivative)}

$f$: $\R^n \rightarrow \R$ an Stelle $a$ nach $x_i$

\[
    \lim_{h \rightarrow 0} \frac{f(a_1, ..., a_i + h, ..., a_n) - f(a_1, ..., a_i, ..., a_n)}{h} =: \frac{\partial f}{\partial x_i}(a)
\]

\textbf{Wichtig}: Alle anderen $x_i$ werden als konstante behandelt bei Ableitung.

\subsubsection{Satz von Schwarz (Higher derivatives)}

$f \in C^2$. Gilt nur für zwei und drei verschiedene Variablen. Beliebige Potenzen von $x_i$ und $x_j$ möglich.

\[
    \frac{\partial^2 f}{\partial x_i x_j} = \frac{\partial^2 f}{\partial x_j x_i} \forall i, j \in \{1, ..., n\}
\]


\subsubsection{Gradient}

\[
    \nabla f =
        \begin{pmatrix}
            \frac{\partial f}{\partial x_1}\\
            \vdots\\
            \frac{\partial f}{\partial x_n}
        \end{pmatrix} \quad \text{Nur für } f: X \rightarrow \mathbb{R}
\]

Gradient eines Skalarfeldes: Richtung: Richtung des steilsten Anstiegs; Betrag: Stärke des Anstiegs.\\

\textbf{Regeln} ($n \in \mathbb{N}$, $c$ konstant, $u, v$ Vektoren):
\begin{align*}
    &\text{grad}(c) = 0 &
    &\text{grad}(c \cdot u) = c \cdot \text{grad}(u) \text{  (Linearität)} \\
    &\text{grad}(u + v) = \text{grad}(u) + \text{grad}(u) \text{  (Addition)} &
    &\text{grad}(u \cdot v) = \text{grad}(u) \cdot \text{grad}(u) 
    \text{  (Produktregel)} \\
    &\text{grad}(u^n) = n \cdot u^{n-1} \cdot \text{grad}(u) (n\neq 0) 
\end{align*}
\subsubsection{Richtungsableitung (Directional Derivative)}

$f$ heisst an der Stelle $a$ in Richtung $u$ differenzierbar, falls der Grenzwert 
	\[
		\lim_{h\to0}\frac{f(a + hu) -f(a)}{h} =
		\left.\frac{d}{dh}f(a+hu)\right|_{h = 0}
		=: D_vf(a)
	\]
existiert (oder $g(h) = f(x_0 + hu)$). Ist $||u|| = 1$ (normiert), heisst dieser Grenzwert Richtungsableitung.

\begin{Rezept}{Richtungsableitung $D_uf(a)$ existiert:}{} Falls $t \mapsto f(a + tu)$ differenzierbar ist bei $t=0$, dann existiert $D_uf(a)$.\\

	Die Richtungsableitung existiert für \textbf{jede Richtung}, falls $\left.\frac{d}{dh} f(a + hu) \right|_{h=0}$ NICHT von $\varphi$ abhängt (wenn mal $x=r\cos(\varphi)$, $y=r\sin(\varphi)$ substituiert).
\end{Rezept}

\begin{Rezept}{Richtungsableitung $D_u f(a)$}{}
Falls $f$ in $a$ differenzierbar ist: Für $f$ in Richtung $u$ in Punkt $a$: 1) $u$ normieren: $\tilde{u} = \frac{u}{\|u\|}$ 2) Gradient $\nabla f(x)$ berechnen, dann:

\[
    D_u f(a) = \tilde{u} \underbrace{\cdot}_{\text{SP.}} \nabla f(a)
\]
\end{Rezept}
\begin{Rezept}{Change of variable}{}
$g$ drückt jede Variable $x_i$ von $f$ mit neuen Variablen $(y_1, ..., y_n)$ aus.
\[
U \subset \mathbb{R}^n \text{ offen } \xrightarrow{g}  V \subset \mathbb{R}^n \text{ offen }
\]
Ableitungen:
\[
\frac{\partial h (y)}{\partial y_i} = \frac{\partial f(g(y)))}{\partial x_1} \cdot \frac{\partial g_1(y)}{\partial y_i} \cdots \frac{\partial f(g(y))}{\partial x_n} \cdot \frac{\partial g_n(y)}{\partial y_i}
\]
In der Notation wird $h$ durch $f$ ersetzt und $g_i$ durch $x_i$.
\end{Rezept}
Beispiel Polarkoordinaten:
\[
g(\iota, \theta) = (\iota \cos{\theta}, \iota \sin{\theta}) \quad
J_g(\iota, \theta) = \begin{pmatrix}
\cos{\theta} & -\iota \sin{\theta} \\ \sin{\theta} & \iota \cos{\theta}
\end{pmatrix}
\]
\[
J_h = J_f \cdot J_g \Rightarrow \partial_\iota h = \dots, \partial_\theta h = \dots \rightarrow \text{Nach $\partial_x f$ und $\partial_y f$ auflösen}
\]

\begin{Diverses}{Koordinatentransformationen}{}
    \textbf{Polarkoordinaten:}
    $\partial_x f = \cos{\theta} \partial_\iota f - \frac{1}{\iota}\sin{\theta}\partial_\theta f \quad
    \partial_x f = \cos{\theta} \partial_\iota f - \frac{1}{\iota}\sin{\theta}\partial_\theta f $\\
    \textbf{Zylinderkoordinaten:} \dots
\end{Diverses}

\begin{Definition}{Hessematrix}{}
\[
    \mathbf{Hess}(f) =
        \begin{pmatrix}
            \frac{\partial^2 f}{\partial x_1^2}&\hdots&\frac{\partial^2 f}{\partial x_1 x_n}\\
            \vdots&\ddots&\vdots\\
            \frac{\partial^2 f}{\partial x_n x_1}&\hdots&\frac{\partial^2 f}{\partial x_n^2}
        \end{pmatrix}
\]
\end{Definition}

\begin{Definition}{Jakobimatrix}{}
\[
    f(x, y) =
        \begin{pmatrix}
            f_1(x, y)\\
            \vdots\\
            f_n(x, y)
        \end{pmatrix} \quad
    \mathbf{J_f}(x, y) =
        \begin{pmatrix}
                \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y}\\
                \vdots&\vdots\\
            \frac{\partial f_n}{\partial x} & \frac{\partial f_n}{\partial y}
        \end{pmatrix}
\]
Für die Kettenregel von Jakobimatrizen siehe ''generelle Kettenregel''.
\end{Definition}

\begin{Definition}{Laplace Operator}{}
\[
\Delta f = \nabla(\nabla f)) = \sum_{k=1}^{n} \frac{\partial^2 f}{\partial x_k^2} = \text{trace}(\textbf{Hess}(f))
\]
\end{Definition}

\subsection{Taylorpolynome}

\[
    f(x) = f(a) + f'(a)(x-a) + \frac{1}{2} f''(a)(x-a)^2 + \frac{1}{3!} f^{(3)}(a)(x-a)^3 + ...
\]

\textbf{In $\R^2$}: $\Delta x = (x - x_0)$, ~ $\Delta y = (y - y_0)$
\begin{align*}
    \; & f(x, y) =f(x_0, y_0)\\ &+ \frac{\partial f}{\partial x}(x_0,y_0) \Delta x + \frac{\partial f}{\partial y}(x_0,y_0) \Delta y\\
    &+ \frac{1}{2} \left(\frac{\partial^2 f}{\partial x^2}(x_0,y_0) (\Delta x)^2 + 2\frac{\partial^2 f}{\partial x \partial y}(x_0,y_0) \Delta x \Delta y + \frac{\partial^2 f}{\partial y^2}(x_0,y_0) (\Delta y)^2\right)\\
    &+ \frac{1}{3!} \left(\frac{\partial^3 f}{\partial x^3}(x_0,y_0) (\Delta x)^3 + 3\frac{\partial^3 f}{\partial x^2 \partial y}(x_0,y_0) (\Delta x)^2 \Delta y\right.\\
    &\quad\quad\quad\;+ \left.3\frac{\partial^3 f}{\partial x \partial y^2}(x_0,y_0) \Delta x (\Delta y)^2 + \frac{\partial^3 f}{\partial y^3}(x_0,y_0) (\Delta y)^3\right)\\
    & + ...
\end{align*}

\textbf{In $\R^n$}: $\Delta x_i = x_i - x_i^0$
\[
	Tf(x;x_0) = \left.\sum_{n=0}^\infty \frac{1}{n!}\left(\Delta x_1 \frac{\partial}{\partial x_1} + ... + \Delta x_n \frac{\partial}{\partial x_n} \right)^n 
		f(x_1, ..., x_n)\right|_{(x_1^0,...,x_n^0)}
\]

\textbf{In $\R^n$ bis Grad 2}:
\[
    T_2f(x) = f(x_0) + \nabla f(x_0)	 (x-x_0) + \frac{1}{2} (x-x_0)^\top \text{Hess}_f(x_0) (x-x_0) 
\]

\begin{Diverses}{Taylorentwicklungen}{}
    \begin{align*}
    e^z &= \sum_{k=0}^{\infty} \frac{z^k}{k!} = 1 + z + \frac{z^2}{2}+ \frac{z^3}{3!}+ \frac{z^4}{4!} + \cdots & 
    \frac{1}{1 \pm x} &= 1 \mp x + x^2 \mp x^3 + x^4 \mp \cdots\\
    \frac{1}{(1 \pm x)^2} &= 1 \mp 2x + 3x^2 \mp 4x^3 + 5x^4 \mp \cdots &
    \sqrt{1 \pm x} &= 1 \pm \frac{x}{2} - \frac{\scriptstyle{1\cdot 1}}{\scriptstyle{2 \cdot 4}}x^2 \pm \frac{\scriptstyle{1\cdot 1 \cdot 3}}{\scriptstyle{2 \cdot 4 \cdot 6}}x^3 - \frac{\scriptstyle{1 \cdot 1 \cdot 3 \cdot 5}}{\scriptstyle{2 \cdot 4 \cdot 6 \cdot 8}}x^4 \pm \scriptstyle\cdots \\
    \sin(\phi) &= \sum_{k=0}^{\infty} (-1)^k \frac{\phi^{2k}}{(2k)!} = \phi - \frac{\phi^3}{3!} + \frac{\phi^5}{5!} + \cdots &
    \sinh(z) &= \sum_{k=0}^{\infty} \frac{z^{2k+1}}{(2k+1)!} = z + \frac{z^3}{3!} + \frac{z^5}{5!} + \cdots\\
    \cos(\phi) &= \sum_{k=0}^{\infty} (-1)^k \frac{\phi^{2k+1}}{(2k+1)!} = 1 - \frac{\phi^2}{2!} + \frac{\phi^4}{4!} + \cdots &
    \cosh(z) &= \sum_{k=0}^{\infty} \frac{z^{2k}}{(2k)!} = 1 + \frac{z^2}{2!} + \frac{z^4}{4!} + \cdots\\
    \tan(\phi) &= \text{...complicated...} = 1 + \frac{\phi^3}{3} + \frac{2\phi^5}{15} + \cdots&
    \tanh(z) &= \text{...complicated...} = 1 \pmb{-} \frac{z^3}{3} \pmb{+} \frac{2z^5}{15} \pmb{-} \cdots\\
    \ln(1+z) &= \sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k}z^k = z - \frac{z^2}{2} + \frac{z^3}{3} + \scriptstyle\cdots &
    (1+z)^\alpha& = \sum_{k=0}^{\infty}  \binom{\alpha}{k} z^k = 1 + \alpha z + \frac{\alpha(\alpha - 1)}{2!} z ^ 2 + \scriptstyle\cdots
    \end{align*}
\end{Diverses}

\begin{Rezept}{Tangentialebene (Variante I)}{}
    Tangentialebene der Fläche $f(x, y)$ finden in Punkt $(x_0, y_0)$.
    
    \textbf{Lösungsschritt I:}
    Erstelle $F(x, y) = (x, y, f(x, y))^\top$ und berechne die
    Basisvektoren für die Tangentialebene
    \[
        dF(x_0, y_0) =
            \begin{pmatrix}
                \frac{\partial F_1}{\partial x}(x_0, y_0)&\frac{\partial F_1}{\partial y}(x_0, y_0)\\
                \frac{\partial F_2}{\partial x}(x_0, y_0)&\frac{\partial F_2}{\partial y}(x_0, y_0)\\
                \frac{\partial F_3}{\partial x}(x_0, y_0)&\frac{\partial F_3}{\partial y}(x_0, y_0)
            \end{pmatrix} = 
            \begin{pmatrix}
                u_1&v_1\\
                u_2&v_2\\
                u_3&v_3
            \end{pmatrix}
    \]
    ($x_0$ und $y_0$ einsetzen).\\
    \textbf{Lösungsschritt II:}
    Berechne Normalvektor:
    \[
        n =
        u \times v =
        \begin{pmatrix}
            u_1\\
            u_2\\
            u_3
        \end{pmatrix}
        \times
        \begin{pmatrix}
            v_1\\
            v_2\\
            v_3
        \end{pmatrix} = 
        \begin{pmatrix}
            a\\
            b\\
            c
        \end{pmatrix}
    \]
    Berechne $d$ mit $p=(x_0,y_0,f(x_0,y_0))$:
    \[
        d=p \cdot n
    \]
    \textbf{Lösungsschritt III:} Konstruiere Gleichung, sodass:
    \[
        ax + by + cz = d
    \]
\end{Rezept}

\begin{Rezept}{Tangentialebene (Variante II)}{}
\textbf{Idee}: Annäherung von $f$ im Punkt $(x_0, y_0)$ durch Taylorpolynom I. Grades
\[ z = f(x_0, y_0) + \frac{\partial f}{\partial x}(x_0, y_0)\cdot(x-x_0) + \frac{\partial f}{\partial y}(x_0, y_0)\cdot(y-y_0) \]
kann direkt umgeformt werden in die Normalform der Ebenengleichung.
Allgemein für diffbare $f: X \rightarrow \mathbb{R}$ auf $x_0 \in \mathbb{R}^n$:
\begin{equation*}
    z = f(x_0) + J_f(x_0) \underbrace{\cdot}_{\small{\text{Matrixprod.}}} (x-x_0)
\end{equation*}
\end{Rezept}
